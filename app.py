import json
import os
import random
import re
from collections import Counter
from pathlib import Path
from typing import Dict, List, Optional

import altair as alt
import pandas as pd
import streamlit as st
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


BASE_DIR = Path(__file__).parent
MODEL_DIR = BASE_DIR / "model"
HEURISTICS_FILE = MODEL_DIR / os.getenv("IMDB_MODEL_FILE", "heuristics.json")
DATASET_DIR = BASE_DIR / "IMDbå½±è©•"
DATASET_FILE = DATASET_DIR / "reviews.jsonl"
DEFAULT_TEMPERATURE = 0.2
STOPWORDS = {
    "the",
    "a",
    "an",
    "of",
    "and",
    "to",
    "is",
    "it",
    "in",
    "that",
    "for",
    "this",
    "on",
    "with",
    "as",
    "was",
    "but",
    "are",
    "be",
    "at",
    "by",
    "from",
    "or",
    "so",
}
WORD_RE = re.compile(r"[A-Za-z']+")


@st.cache_resource(show_spinner=False)
def load_local_model():
    if not HEURISTICS_FILE.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°æ¨¡å‹è¨­å®šæª”ï¼š{HEURISTICS_FILE}. è«‹ç¢ºèª model è³‡æ–™å¤¾å·²åŒæ­¥ã€‚"
        )
    with open(HEURISTICS_FILE, "r", encoding="utf-8") as fp:
        heuristics = json.load(fp)
    analyzer = SentimentIntensityAnalyzer()
    return heuristics, analyzer


@st.cache_data(show_spinner=False)
def load_review_dataset() -> List[Dict[str, str]]:
    if not DATASET_FILE.exists():
        return []
    rows: List[Dict[str, str]] = []
    with open(DATASET_FILE, "r", encoding="utf-8") as fp:
        for line in fp:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows


def set_review_context(text: str, movie: Optional[str] = None):
    st.session_state["input_review"] = text
    if movie:
        st.session_state["movie_title"] = movie


def set_random_review(rows: List[Dict[str, str]]):
    if rows:
        choice = random.choice(rows)
        set_review_context(choice["review"], choice.get("movie") or choice.get("title"))


def build_chart_dataframe(rows: List[Dict[str, str]], field: str):
    counter = Counter(row.get(field, "Unknown") for row in rows)
    if not counter:
        return None
    data = pd.DataFrame(
        {"label": list(counter.keys()), "count": list(counter.values())}
    ).sort_values("count", ascending=False)
    return data


def tokenize(text: str) -> List[str]:
    return [w.lower() for w in WORD_RE.findall(text)]


def detect_sentiment_label(
    text: str, compound: float, keywords: Dict[str, List[str]]
) -> str:
    lowered = text.lower()
    for label, cues in keywords.items():
        if any(cue in lowered for cue in cues):
            return label
    if compound >= 0.35:
        return "Positive"
    if compound <= -0.35:
        return "Negative"
    if abs(compound) <= 0.1:
        return "Neutral"
    return "Disappointed" if compound < 0 else "Touched"


def detect_topic(text: str, topic_keywords: Dict[str, List[str]]) -> str:
    lowered = text.lower()
    scores = {}
    for topic, cues in topic_keywords.items():
        scores[topic] = sum(lowered.count(cue) for cue in cues)
    best_topic = max(scores, key=scores.get)
    return best_topic if scores[best_topic] > 0 else "Other"


def select_keywords(tokens: List[str], top_n: int) -> List[str]:
    filtered = [t for t in tokens if t not in STOPWORDS and len(t) > 2]
    counts = Counter(filtered)
    return [word for word, _ in counts.most_common(top_n)]


def split_sentences(text: str) -> List[str]:
    sentences = re.split(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+", text.strip())
    return [s.strip() for s in sentences if s.strip()]


def score_sentence(
    sentence: str, analyzer: SentimentIntensityAnalyzer, diversity: float
) -> float:
    tokens = tokenize(sentence)
    if not tokens:
        return 0.0
    sentiment = abs(analyzer.polarity_scores(sentence)["compound"])
    return len(tokens) + sentiment * (5 + diversity * 5)


def summarize(
    sentences: List[str], analyzer: SentimentIntensityAnalyzer, diversity: float
) -> List[str]:
    scored = sorted(
        ((score_sentence(s, analyzer, diversity), s) for s in sentences),
        key=lambda pair: pair[0],
        reverse=True,
    )
    return [s for _, s in scored[: min(3, len(scored))]]


def calc_intensity(compound: float) -> float:
    return round(min(1.0, abs(compound)) * 10, 2)


def rating_from_sentiment(compound: float) -> float:
    rating = ((compound + 1) / 2) * 10
    return round(max(0, min(10, rating)), 1)


def audience_suggestion(text: str, profiles: List[Dict[str, List[str]]]) -> str:
    lowered = text.lower()
    best_label = "é©åˆå°‹æ‰¾åŠ‡æƒ…/è§’è‰²æ·±åº¦çš„è§€çœ¾ï¼Œé¿å…æœŸå¾…çˆ†ç±³èŠ±å¨›æ¨‚çš„äººã€‚"
    best_score = 0
    for profile in profiles:
        score = sum(lowered.count(keyword) for keyword in profile["keywords"])
        if score > best_score:
            best_score = score
            best_label = profile["label"]
    return best_label


def analyze_review(review: str, diversity: float, keyword_top_n: int):
    heuristics, analyzer = load_local_model()
    text = review.strip()
    compound = analyzer.polarity_scores(text)["compound"]
    sentiment_label = detect_sentiment_label(
        text, compound, heuristics["sentiment_keywords"]
    )
    topic = detect_topic(text, heuristics["topic_keywords"])
    sentences = split_sentences(text)
    highlight_sentences = summarize(sentences, analyzer, diversity)
    summary = " ".join(highlight_sentences) if highlight_sentences else text
    tokens = tokenize(text)
    keywords = select_keywords(tokens, keyword_top_n)
    return {
        "sentiment_label": sentiment_label,
        "topic": topic,
        "summary": summary,
        "sentiment_score_10": calc_intensity(compound),
        "key_sentences": highlight_sentences,
        "keywords": keywords,
        "rating_pred_10": rating_from_sentiment(compound),
        "audience_suitability": audience_suggestion(
            text, heuristics["audience_profiles"]
        ),
    }


def render_results(parsed: dict, movie_name: str):
    st.subheader("åˆ†æçµæœ")
    st.caption(f"ğŸï¸ å½±è©•ä¾†æºï¼š{movie_name or 'æœªæŒ‡å®šé›»å½±'}")
    col1, col2 = st.columns(2)

    col1.metric("æƒ…ç·’/å¿ƒæƒ… (7é¡)", parsed.get("sentiment_label", "N/A"))
    col1.metric("ä¸»é¡Œ", parsed.get("topic", "N/A"))
    col1.metric("æƒ…ç·’å¼·åº¦ /10", f"{parsed.get('sentiment_score_10', 'N/A')}")
    col1.metric("å¯èƒ½è©•åˆ† /10", f"{parsed.get('rating_pred_10', 'N/A')}")

    col2.write("**æ‘˜è¦ï¼ˆä¿ç•™æƒ…ç·’ï¼‰**")
    col2.write(parsed.get("summary", ""))

    col2.write("**è§€çœ¾é©é…**")
    col2.write(parsed.get("audience_suitability", ""))

    st.write("---")
    st.write("**é—œéµå¥**")
    key_sentences = parsed.get("key_sentences") or []
    for idx, sentence in enumerate(key_sentences, 1):
        st.write(f"{idx}. {sentence}")

    st.write("**é—œéµå­—**")
    keywords = parsed.get("keywords") or []
    st.write(", ".join(keywords))


def main():
    st.set_page_config(
        page_title="IMDB Review 7åˆ1 æƒ…ç·’/ä¸»é¡Œ/æ‘˜è¦åˆ†æ",
        page_icon="ğŸ¬",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.title("ğŸ¬ IMDB å½±è©• 7 åˆ 1 æ™ºèƒ½åˆ†æ")
    st.caption(
        "å¤šé¡æƒ…ç·’ã€ä¸»é¡Œã€æƒ…ç·’ä¿ç•™æ‘˜è¦ã€å¼·åº¦ã€é—œéµå¥/è©ã€è©•åˆ†èˆ‡è§€çœ¾é©é…ï¼Œä¸€æ¬¡å®Œæˆã€‚"
    )

    dataset_rows = load_review_dataset()
    filtered_rows = dataset_rows
    chart_field = "sentiment"
    chart_use_filter = True
    show_raw_table = False
    default_text = (
        "The film's pacing is uneven, but the acting is heartfelt. "
        "I laughed a few times, yet the ending felt rushed and predictable. "
        "Overall, it's a decent weekend watch, nothing mind-blowing."
    )
    if "input_review" not in st.session_state:
        st.session_state.input_review = default_text
    if "movie_title" not in st.session_state:
        st.session_state.movie_title = "Custom Review"

    with st.sidebar:
        st.header("æ¨ç†è¨­å®š")
        temperature = st.slider(
            "èª¿æ•´æƒ…ç·’éˆæ•åº¦ï¼ˆåƒ…å½±éŸ¿é—œéµå¥æ’åºï¼‰",
            min_value=0.0,
            max_value=1.0,
            value=DEFAULT_TEMPERATURE,
            step=0.05,
            help="è¶Šé«˜è¡¨ç¤ºæ›´åå¥½æƒ…ç·’æ³¢å‹•å¤§çš„å¥å­ï¼Œ0 å‰‡åå¥½é—œéµè³‡è¨Šã€‚",
        )
        keyword_top_n = st.slider(
            "é¡¯ç¤ºçš„é—œéµå­—æ•¸é‡",
            min_value=3,
            max_value=10,
            value=6,
            step=1,
        )
        st.markdown(
            f"æ¨¡å‹ï¼š`{HEURISTICS_FILE}`ï¼ˆæœ¬åœ°è¦å‰‡/å­—å…¸ï¼Œç„¡éœ€é›²ç«¯ä¸‹è¼‰ï¼‰"
        )
        st.markdown(
            f"è³‡æ–™é›†ï¼š`{DATASET_FILE if DATASET_FILE.exists() else 'å°šæœªæä¾›'}`"
        )
        st.divider()

        st.subheader("æ¸¬è©¦ç´ æå·¥å…·")
        dataset_count = len(dataset_rows)
        st.metric("å…§å»ºå½±è©•æ•¸", dataset_count)
        if dataset_rows:
            sentiments = Counter(row["sentiment"] for row in dataset_rows)
            top_sentiments = ", ".join(
                f"{label}:{count}" for label, count in sentiments.most_common(3)
            )
            st.caption(f"å¸¸è¦‹æƒ…ç·’ï¼š{top_sentiments or 'N/A'}")
            sentiment_options = ["å…¨éƒ¨"] + sorted(sentiments.keys())
            sentiment_filter = st.selectbox(
                "æƒ…ç·’ç¯©é¸", sentiment_options, key="sentiment_filter"
            )
            rating_min, rating_max = st.slider(
                "è©•åˆ†å€é–“",
                min_value=0.0,
                max_value=10.0,
                value=(0.0, 10.0),
                step=0.5,
            )
            filtered_rows = [
                row
                for row in dataset_rows
                if (sentiment_filter == "å…¨éƒ¨" or row["sentiment"] == sentiment_filter)
                and rating_min <= float(row.get("rating", 0)) <= rating_max
            ]
            if not filtered_rows:
                st.info("ç¯©é¸æ¢ä»¶ä¸‹æ²’æœ‰å°æ‡‰çš„å½±è©•ï¼Œå°‡æ”¹ç”¨å…¨éƒ¨è³‡æ–™ã€‚")
            sample_source = filtered_rows if filtered_rows else dataset_rows

            sample_map = {
                f"{row.get('id','?')} Â· {row.get('movie', row.get('title','N/A'))}": row
                for row in sample_source
            }
            sample_label = st.selectbox(
                "æŒ‘é¸å…§å»ºå½±è©•",
                list(sample_map.keys()),
                key="sample_selector",
            )
            chosen_sample = sample_map[sample_label]
            st.caption(
                f"é›»å½±ï¼š{chosen_sample.get('movie','N/A')} Â· æƒ…ç·’ï¼š{chosen_sample['sentiment']} Â· ä¸»é¡Œï¼š{chosen_sample['topic']} Â· è©•åˆ†ï¼š{chosen_sample['rating']}"
            )
            col_load, col_random = st.columns(2)
            col_load.button(
                "è¼‰å…¥é¸æ“‡é …",
                use_container_width=True,
                key="btn_load_selected",
                on_click=set_review_context,
                args=(chosen_sample["review"], chosen_sample.get("movie")),
            )
            col_random.button(
                "éš¨æ©ŸæŠ½æ¨£å½±è©•",
                use_container_width=True,
                key="btn_random_sample",
                on_click=set_random_review,
                args=(sample_source,),
            )
            with st.expander("é è¦½é¸å®šå½±è©•"):
                st.markdown(f"**{chosen_sample.get('movie','N/A')}**")
                st.write(chosen_sample["review"])
            dataset_text = "\n".join(
                json.dumps(row, ensure_ascii=False) for row in dataset_rows
            )
            st.download_button(
                "ä¸‹è¼‰æ¨£æœ¬ JSONL",
                data=dataset_text,
                file_name="imdb_reviews_samples.jsonl",
                mime="application/json",
                use_container_width=True,
            )
            chart_field = st.radio(
                "è³‡æ–™è¦–è¦ºåŒ–é …ç›®",
                options=["sentiment", "topic"],
                format_func=lambda x: "æƒ…ç·’åˆ†ä½ˆ" if x == "sentiment" else "ä¸»é¡Œåˆ†ä½ˆ",
                key="chart_field_radio",
            )
            chart_use_filter = st.checkbox(
                "åœ–è¡¨ä½¿ç”¨ç¯©é¸çµæœ", value=True, key="chart_use_filter"
            )
            show_raw_table = st.checkbox(
                "é¡¯ç¤ºè³‡æ–™è¡¨", value=False, key="show_raw_table"
            )
        else:
            st.info("å°šæœªæ‰¾åˆ° `IMDbå½±è©•/reviews.jsonl`ï¼Œåƒ…èƒ½æ‰‹å‹•è¼¸å…¥å½±è©•ã€‚")

    review = st.text_area(
        "è²¼ä¸Š IMDB å½±è©•æ–‡å­—",
        key="input_review",
        height=200,
        placeholder="è¼¸å…¥è‹±æ–‡æˆ–ä¸­è‹±æ··åˆå½±è©•ï¼ŒæŒ‰ä¸‹åˆ†æé–‹å§‹ã€‚",
    )
    movie_name = st.text_input(
        "é›»å½±åç¨±",
        key="movie_title",
        placeholder="è¼¸å…¥æ­£åœ¨åˆ†æçš„é›»å½±åç¨±",
        help="å¦‚å¾è³‡æ–™é›†ä¸­é¸å–æœƒè‡ªå‹•å¸¶å…¥ï¼Œäº¦å¯è‡ªè¡Œä¿®æ”¹ã€‚",
    )

    quick_source = filtered_rows if filtered_rows else dataset_rows
    if quick_source:
        st.caption("å¿«é€Ÿå¥—ç”¨æ¸¬è©¦å½±è©•ï¼š")
        quick_samples = []
        sentiments_to_show = ["Positive", "Negative", "Neutral"]
        for sentiment in sentiments_to_show:
            match = next(
                (row for row in quick_source if row["sentiment"] == sentiment),
                None,
            )
            if match:
                quick_samples.append(match)
        if not quick_samples:
            quick_samples = quick_source[: min(3, len(quick_source))]
        cols = st.columns(len(quick_samples))
        for col, sample in zip(cols, quick_samples):
            label = f"{sample.get('movie','N/A')} Â· {sample['sentiment']}"
            col.button(
                label,
                use_container_width=True,
                key=f"quick_{sample['id']}",
                on_click=set_review_context,
                args=(sample["review"], sample.get("movie")),
            )

    if st.button("é–‹å§‹åˆ†æ", type="primary"):
        if not review.strip():
            st.warning("è«‹å…ˆè¼¸å…¥å½±è©•æ–‡å­—ã€‚")
        else:
            with st.spinner("æœ¬åœ°æ¨¡å‹åˆ†æä¸­ï¼Œè«‹ç¨å€™..."):
                parsed = analyze_review(review, temperature, keyword_top_n)
            render_results(parsed, movie_name.strip() or "æœªå‘½åå½±è©•")

    chart_source = []
    if dataset_rows:
        chart_source = quick_source if chart_use_filter else dataset_rows
        chart_df = build_chart_dataframe(chart_source, chart_field)
        chart_labels = {"sentiment": "æƒ…ç·’åˆ†ä½ˆ", "topic": "ä¸»é¡Œåˆ†ä½ˆ"}
        if chart_df is not None:
            st.subheader(f"å½±è©•è³‡æ–™åœ– Â· {chart_labels.get(chart_field, chart_field)}")
            chart_data = chart_df.rename(columns={"label": "åˆ†é¡", "count": "ç¯‡æ•¸"})
            chart = (
                alt.Chart(chart_data)
                .mark_bar(cornerRadiusTopLeft=6, cornerRadiusTopRight=6)
                .encode(
                    x=alt.X("åˆ†é¡:N", sort="-y", title="åˆ†é¡"),
                    y=alt.Y("ç¯‡æ•¸:Q", title="ç¯‡æ•¸"),
                    color=alt.Color(
                        "åˆ†é¡:N",
                        legend=None,
                        scale=alt.Scale(scheme="teals"),
                    ),
                    tooltip=["åˆ†é¡:N", "ç¯‡æ•¸:Q"],
                )
                .properties(height=320)
            )
            st.altair_chart(chart, use_container_width=True)
        if show_raw_table and chart_source:
            st.dataframe(
                pd.DataFrame(chart_source)[
                    ["id", "title", "sentiment", "topic", "rating"]
                ]
            )


if __name__ == "__main__":
    main()

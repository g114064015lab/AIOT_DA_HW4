import json
import math
import os
import random
import re
from collections import Counter
from pathlib import Path
from typing import Dict, List, Tuple

import altair as alt
import pandas as pd
import streamlit as st
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


BASE_DIR = Path(__file__).parent
MODEL_DIR = BASE_DIR / "model"
HEURISTICS_FILE = MODEL_DIR / os.getenv("IMDB_MODEL_FILE", "heuristics.json")
DATASET_DIR = BASE_DIR / "IMDbå½±è©•"
DATASET_FILE = DATASET_DIR / "reviews.jsonl"
DEFAULT_TEMPERATURE = 0.2
DEFAULT_RATING_MAPPING = (5.0, 5.0)  # slope, intercept
STOPWORDS = {
    "the",
    "a",
    "an",
    "of",
    "and",
    "to",
    "is",
    "it",
    "in",
    "that",
    "for",
    "this",
    "on",
    "with",
    "as",
    "was",
    "but",
    "are",
    "be",
    "at",
    "by",
    "from",
    "or",
    "so",
}
WORD_RE = re.compile(r"[A-Za-z']+")


def rating_value(row: Dict[str, str]) -> float:
    try:
        return float(row.get("rating", 0))
    except (TypeError, ValueError):
        return 0.0


@st.cache_resource(show_spinner=False)
def load_local_model():
    if not HEURISTICS_FILE.exists():
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°æ¨¡å‹è¨­å®šæª”ï¼š{HEURISTICS_FILE}. è«‹ç¢ºèª model è³‡æ–™å¤¾å·²åŒæ­¥ã€‚"
        )
    with open(HEURISTICS_FILE, "r", encoding="utf-8") as fp:
        heuristics = json.load(fp)
    analyzer = SentimentIntensityAnalyzer()
    return heuristics, analyzer


@st.cache_data(show_spinner=False)
def load_review_dataset() -> List[Dict[str, str]]:
    if not DATASET_FILE.exists():
        return []
    rows: List[Dict[str, str]] = []
    with open(DATASET_FILE, "r", encoding="utf-8") as fp:
        for line in fp:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows


@st.cache_data(show_spinner=False)
def compute_rating_mapping(rows: List[Dict[str, str]]) -> Tuple[float, float]:
    analyzer = SentimentIntensityAnalyzer()
    samples = []
    for row in rows:
        review = row.get("review")
        rating = row.get("rating")
        if not review or rating is None:
            continue
        try:
            rating_float = float(rating)
        except (TypeError, ValueError):
            continue
        compound = analyzer.polarity_scores(review)["compound"]
        samples.append((compound, rating_float))
    if len(samples) < 2:
        return DEFAULT_RATING_MAPPING
    mean_x = sum(x for x, _ in samples) / len(samples)
    mean_y = sum(y for _, y in samples) / len(samples)
    numerator = sum((x - mean_x) * (y - mean_y) for x, y in samples)
    denominator = sum((x - mean_x) ** 2 for x, _ in samples)
    if denominator == 0:
        return DEFAULT_RATING_MAPPING
    slope = numerator / denominator
    intercept = mean_y - slope * mean_x
    if not (math.isfinite(slope) and math.isfinite(intercept)):
        return DEFAULT_RATING_MAPPING
    return slope, intercept


def set_review_text(text: str):
    st.session_state["input_review"] = text


def set_random_review(rows: List[Dict[str, str]]):
    if rows:
        set_review_text(random.choice(rows)["review"])


def build_chart_dataframe(rows: List[Dict[str, str]], field: str):
    counter = Counter(row.get(field, "Unknown") for row in rows)
    if not counter:
        return None
    data = pd.DataFrame(
        {"label": list(counter.keys()), "count": list(counter.values())}
    ).sort_values("count", ascending=False)
    return data


def tokenize(text: str) -> List[str]:
    return [w.lower() for w in WORD_RE.findall(text)]


def detect_sentiment_label(
    text: str, compound: float, keywords: Dict[str, List[str]]
) -> str:
    lowered = text.lower()
    for label, cues in keywords.items():
        if any(cue in lowered for cue in cues):
            return label
    if compound >= 0.35:
        return "Positive"
    if compound <= -0.35:
        return "Negative"
    if abs(compound) <= 0.1:
        return "Neutral"
    return "Disappointed" if compound < 0 else "Touched"


def detect_topic(text: str, topic_keywords: Dict[str, List[str]]) -> str:
    lowered = text.lower()
    scores = {}
    for topic, cues in topic_keywords.items():
        scores[topic] = sum(lowered.count(cue) for cue in cues)
    best_topic = max(scores, key=scores.get)
    return best_topic if scores[best_topic] > 0 else "Other"


def select_keywords(tokens: List[str], top_n: int) -> List[str]:
    filtered = [t for t in tokens if t not in STOPWORDS and len(t) > 2]
    counts = Counter(filtered)
    return [word for word, _ in counts.most_common(top_n)]


def split_sentences(text: str) -> List[str]:
    sentences = re.split(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+", text.strip())
    return [s.strip() for s in sentences if s.strip()]


def score_sentence(
    sentence: str, analyzer: SentimentIntensityAnalyzer, diversity: float
) -> float:
    tokens = tokenize(sentence)
    if not tokens:
        return 0.0
    sentiment = abs(analyzer.polarity_scores(sentence)["compound"])
    return len(tokens) + sentiment * (5 + diversity * 5)


def summarize(
    sentences: List[str], analyzer: SentimentIntensityAnalyzer, diversity: float
) -> List[str]:
    scored = sorted(
        ((score_sentence(s, analyzer, diversity), s) for s in sentences),
        key=lambda pair: pair[0],
        reverse=True,
    )
    return [s for _, s in scored[: min(3, len(scored))]]


def calc_intensity(compound: float) -> float:
    return round(min(1.0, abs(compound)) * 10, 2)


def rating_from_sentiment(
    compound: float, mapping: Tuple[float, float] = DEFAULT_RATING_MAPPING
) -> float:
    slope, intercept = mapping
    rating = slope * compound + intercept
    return round(max(0, min(10, rating)), 1)


def audience_suggestion_from_rating(score: float) -> str:
    buckets = [
        (8.5, 11.0, "æ¨è–¦å°‹æ‰¾é«˜å“è³ªæ•˜äº‹èˆ‡æ¼”æŠ€çš„è§€çœ¾ï¼Œå¹¾ä¹å¯æ”¾å¿ƒè¡é¦–è¼ªã€‚"),
        (7.0, 8.5, "é©åˆè©²é¡Œæç²‰çµ²æˆ–æƒ³æ‰¾ç²¾ç·»å¨›æ¨‚çš„è§€çœ¾ï¼Œæˆ²é™¢æˆ–ä¸²æµéƒ½å€¼å¾—ã€‚"),
        (5.0, 7.0, "å»ºè­°è¼•é¬†çœ‹ç‰‡æˆ–ä¸²æµè§€è³ï¼ŒæŒ‘ç´°ç¯€çš„è§€çœ¾å¯èƒ½è¦ºå¾—æ™®é€šã€‚"),
        (3.0, 5.0, "åƒ…æ¨è–¦éµç²‰æˆ–æƒ³åæ§½çš„è§€çœ¾ï¼Œå…¶ä»–äººå¯æ–Ÿé…Œæ™‚é–“æˆæœ¬ã€‚"),
        (0.0, 3.0, "å¤šæ•¸è§€çœ¾ç›´æ¥è·³éè¼ƒä½³ï¼Œå¯æŠŠæ™‚é–“ç•™çµ¦æ›´åˆèƒƒå£çš„ä½œå“ã€‚"),
    ]
    for lower, upper, message in buckets:
        if lower <= score < upper:
            return message
    return "ä¾å€‹äººå“å‘³è‡ªè¡Œæ–Ÿé…Œã€‚"


def analyze_review(
    review: str,
    diversity: float,
    keyword_top_n: int,
    rating_mapping: Tuple[float, float] = DEFAULT_RATING_MAPPING,
):
    heuristics, analyzer = load_local_model()
    text = review.strip()
    compound = analyzer.polarity_scores(text)["compound"]
    sentiment_label = detect_sentiment_label(
        text, compound, heuristics["sentiment_keywords"]
    )
    topic = detect_topic(text, heuristics["topic_keywords"])
    sentences = split_sentences(text)
    highlight_sentences = summarize(sentences, analyzer, diversity)
    summary = " ".join(highlight_sentences) if highlight_sentences else text
    tokens = tokenize(text)
    keywords = select_keywords(tokens, keyword_top_n)
    rating_score = rating_from_sentiment(compound, rating_mapping)
    return {
        "sentiment_label": sentiment_label,
        "topic": topic,
        "summary": summary,
        "sentiment_score_10": calc_intensity(compound),
        "key_sentences": highlight_sentences,
        "keywords": keywords,
        "rating_pred_10": rating_score,
        "audience_suitability": audience_suggestion_from_rating(rating_score),
    }


def render_results(parsed: dict):
    st.subheader("åˆ†æçµæœ")
    col1, col2 = st.columns(2)

    col1.metric("æƒ…ç·’/å¿ƒæƒ… (7é¡)", parsed.get("sentiment_label", "N/A"))
    col1.metric("ä¸»é¡Œ", parsed.get("topic", "N/A"))
    col1.metric("æƒ…ç·’å¼·åº¦ /10", f"{parsed.get('sentiment_score_10', 'N/A')}")
    col1.metric("å¯èƒ½è©•åˆ† /10", f"{parsed.get('rating_pred_10', 'N/A')}")

    col2.write("**æ‘˜è¦ï¼ˆä¿ç•™æƒ…ç·’ï¼‰**")
    col2.write(parsed.get("summary", ""))

    col2.write("**è§€çœ¾é©é…**")
    col2.write(parsed.get("audience_suitability", ""))

    st.write("---")
    st.write("**é—œéµå¥**")
    key_sentences = parsed.get("key_sentences") or []
    for idx, sentence in enumerate(key_sentences, 1):
        st.write(f"{idx}. {sentence}")

    st.write("**é—œéµå­—**")
    keywords = parsed.get("keywords") or []
    st.write(", ".join(keywords))


def main():
    st.set_page_config(
        page_title="IMDB Review 7åˆ1 æƒ…ç·’/ä¸»é¡Œ/æ‘˜è¦åˆ†æ",
        page_icon="ğŸ¬",
        layout="wide",
        initial_sidebar_state="expanded",
    )
    st.title("ğŸ¬ IMDB å½±è©• 7 åˆ 1 æ™ºèƒ½åˆ†æ")
    st.caption(
        "å¤šé¡æƒ…ç·’ã€ä¸»é¡Œã€æƒ…ç·’ä¿ç•™æ‘˜è¦ã€å¼·åº¦ã€é—œéµå¥/è©ã€è©•åˆ†èˆ‡è§€çœ¾é©é…ï¼Œä¸€æ¬¡å®Œæˆã€‚"
    )

    dataset_rows = load_review_dataset()
    filtered_rows = dataset_rows
    chart_field = "sentiment"
    chart_use_filter = True
    show_raw_table = False
    default_text = (
        "The film's pacing is uneven, but the acting is heartfelt. "
        "I laughed a few times, yet the ending felt rushed and predictable. "
        "Overall, it's a decent weekend watch, nothing mind-blowing."
    )
    rating_mapping = (
        compute_rating_mapping(dataset_rows) if dataset_rows else DEFAULT_RATING_MAPPING
    )

    if "input_review" not in st.session_state:
        st.session_state.input_review = default_text

    with st.sidebar:
        st.header("æ¨ç†è¨­å®š")
        temperature = st.slider(
            "èª¿æ•´æƒ…ç·’éˆæ•åº¦ï¼ˆåƒ…å½±éŸ¿é—œéµå¥æ’åºï¼‰",
            min_value=0.0,
            max_value=1.0,
            value=DEFAULT_TEMPERATURE,
            step=0.05,
            help="è¶Šé«˜è¡¨ç¤ºæ›´åå¥½æƒ…ç·’æ³¢å‹•å¤§çš„å¥å­ï¼Œ0 å‰‡åå¥½é—œéµè³‡è¨Šã€‚",
        )
        keyword_top_n = st.slider(
            "é¡¯ç¤ºçš„é—œéµå­—æ•¸é‡",
            min_value=3,
            max_value=10,
            value=6,
            step=1,
        )
        st.markdown(
            f"æ¨¡å‹ï¼š`{HEURISTICS_FILE}`ï¼ˆæœ¬åœ°è¦å‰‡/å­—å…¸ï¼Œç„¡éœ€é›²ç«¯ä¸‹è¼‰ï¼‰"
        )
        st.markdown(
            f"è³‡æ–™é›†ï¼š`{DATASET_FILE if DATASET_FILE.exists() else 'å°šæœªæä¾›'}`"
        )
        st.divider()

        st.subheader("æ¸¬è©¦ç´ æå·¥å…·")
        dataset_count = len(dataset_rows)
        st.metric("å…§å»ºå½±è©•æ•¸", dataset_count)
        if dataset_rows:
            sentiments = Counter(row["sentiment"] for row in dataset_rows)
            top_sentiments = ", ".join(
                f"{label}:{count}" for label, count in sentiments.most_common(3)
            )
            st.caption(f"å¸¸è¦‹æƒ…ç·’ï¼š{top_sentiments or 'N/A'}")
            sentiment_options = ["å…¨éƒ¨"] + sorted(sentiments.keys())
            sentiment_filter = st.selectbox(
                "æƒ…ç·’ç¯©é¸", sentiment_options, key="sentiment_filter"
            )
            rating_min, rating_max = st.slider(
                "è©•åˆ†å€é–“",
                min_value=0.0,
                max_value=10.0,
                value=(0.0, 10.0),
                step=0.5,
            )
            filtered_rows = [
                row
                for row in dataset_rows
                if (sentiment_filter == "å…¨éƒ¨" or row["sentiment"] == sentiment_filter)
                and rating_min <= float(row.get("rating", 0)) <= rating_max
            ]
            if not filtered_rows:
                st.info("ç¯©é¸æ¢ä»¶ä¸‹æ²’æœ‰å°æ‡‰çš„å½±è©•ï¼Œå°‡æ”¹ç”¨å…¨éƒ¨è³‡æ–™ã€‚")
            sample_source = filtered_rows if filtered_rows else dataset_rows

            sample_map = {
                f"{row.get('id','?')} Â· {row.get('title','Untitled')}": row
                for row in sample_source
            }
            sample_label = st.selectbox(
                "æŒ‘é¸å…§å»ºå½±è©•",
                list(sample_map.keys()),
                key="sample_selector",
            )
            chosen_sample = sample_map[sample_label]
            st.caption(
                f"æ¨™é¡Œï¼š{chosen_sample.get('title','Untitled')} Â· æƒ…ç·’ï¼š{chosen_sample['sentiment']} Â· ä¸»é¡Œï¼š{chosen_sample['topic']} Â· è©•åˆ†ï¼š{chosen_sample['rating']}"
            )
            col_load, col_random = st.columns(2)
            col_load.button(
                "è¼‰å…¥é¸æ“‡é …",
                use_container_width=True,
                key="btn_load_selected",
                on_click=set_review_text,
                args=(chosen_sample["review"],),
            )
            col_random.button(
                "éš¨æ©ŸæŠ½æ¨£å½±è©•",
                use_container_width=True,
                key="btn_random_sample",
                on_click=set_random_review,
                args=(sample_source,),
            )
            with st.expander("é è¦½é¸å®šå½±è©•"):
                st.markdown(f"**{chosen_sample.get('title','Untitled')}**")
                st.write(chosen_sample["review"])
            dataset_text = "\n".join(
                json.dumps(row, ensure_ascii=False) for row in dataset_rows
            )
            st.download_button(
                "ä¸‹è¼‰æ¨£æœ¬ JSONL",
                data=dataset_text,
                file_name="imdb_reviews_samples.jsonl",
                mime="application/json",
                use_container_width=True,
            )
            chart_field = st.radio(
                "è³‡æ–™è¦–è¦ºåŒ–é …ç›®",
                options=["sentiment", "topic"],
                format_func=lambda x: "æƒ…ç·’åˆ†ä½ˆ" if x == "sentiment" else "ä¸»é¡Œåˆ†ä½ˆ",
                key="chart_field_radio",
            )
            chart_use_filter = st.checkbox(
                "åœ–è¡¨ä½¿ç”¨ç¯©é¸çµæœ", value=True, key="chart_use_filter"
            )
            show_raw_table = st.checkbox(
                "é¡¯ç¤ºè³‡æ–™è¡¨", value=False, key="show_raw_table"
            )
        else:
            st.info("å°šæœªæ‰¾åˆ° `IMDbå½±è©•/reviews.jsonl`ï¼Œåƒ…èƒ½æ‰‹å‹•è¼¸å…¥å½±è©•ã€‚")

    review = st.text_area(
        "è²¼ä¸Š IMDB å½±è©•æ–‡å­—",
        key="input_review",
        height=200,
        placeholder="è¼¸å…¥è‹±æ–‡æˆ–ä¸­è‹±æ··åˆå½±è©•ï¼ŒæŒ‰ä¸‹åˆ†æé–‹å§‹ã€‚",
    )

    quick_source = filtered_rows if filtered_rows else dataset_rows
    if quick_source:
        st.caption("å¿«é€Ÿå¥—ç”¨æ¸¬è©¦å½±è©•ï¼š")

        def pick_sample(sentiment: str, prefer_highest: bool):
            candidates = [
                row for row in quick_source if row["sentiment"] == sentiment
            ]
            if not candidates:
                return None
            candidates.sort(key=rating_value, reverse=prefer_highest)
            return candidates[0]

        positive_sample = pick_sample("Positive", prefer_highest=True)
        negative_sample = pick_sample("Negative", prefer_highest=False)

        neutral_candidates = [
            row for row in quick_source if row["sentiment"] == "Neutral"
        ]
        neutral_sample = None
        if neutral_candidates:
            neutral_candidates.sort(key=rating_value)
            if positive_sample and negative_sample:
                target = (
                    rating_value(positive_sample) + rating_value(negative_sample)
                ) / 2
            elif positive_sample:
                target = rating_value(positive_sample) * 0.8
            elif negative_sample:
                target = rating_value(negative_sample) + 1.5
            else:
                target = rating_value(
                    neutral_candidates[len(neutral_candidates) // 2]
                )
            neutral_sample = min(
                neutral_candidates, key=lambda row: abs(rating_value(row) - target)
            )
            if (
                positive_sample
                and neutral_sample
                and rating_value(neutral_sample) > rating_value(positive_sample)
            ):
                neutral_sample = next(
                    (
                        row
                        for row in reversed(neutral_candidates)
                        if rating_value(row) <= rating_value(positive_sample)
                    ),
                    neutral_sample,
                )
            if (
                negative_sample
                and neutral_sample
                and rating_value(neutral_sample) < rating_value(negative_sample)
            ):
                neutral_sample = next(
                    (
                        row
                        for row in neutral_candidates
                        if rating_value(row) >= rating_value(negative_sample)
                    ),
                    neutral_sample,
                )

        quick_samples = [
            sample
            for sample in [positive_sample, neutral_sample, negative_sample]
            if sample
        ]
        if not quick_samples:
            quick_samples = quick_source[: min(3, len(quick_source))]
        cols = st.columns(len(quick_samples))
        for col, sample in zip(cols, quick_samples):
            label = (
                f"{sample['sentiment']} Â· {sample.get('title','Untitled')} "
                f"({rating_value(sample):.1f}/10)"
            )
            col.button(
                label,
                use_container_width=True,
                key=f"quick_{sample['id']}",
                on_click=set_review_text,
                args=(sample["review"],),
            )

    if st.button("é–‹å§‹åˆ†æ", type="primary"):
        if not review.strip():
            st.warning("è«‹å…ˆè¼¸å…¥å½±è©•æ–‡å­—ã€‚")
        else:
            with st.spinner("æœ¬åœ°æ¨¡å‹åˆ†æä¸­ï¼Œè«‹ç¨å€™..."):
                parsed = analyze_review(
                    review, temperature, keyword_top_n, rating_mapping
                )
            render_results(parsed)

    chart_source = []
    if dataset_rows:
        chart_source = quick_source if chart_use_filter else dataset_rows
        chart_df = build_chart_dataframe(chart_source, chart_field)
        chart_labels = {"sentiment": "æƒ…ç·’åˆ†ä½ˆ", "topic": "ä¸»é¡Œåˆ†ä½ˆ"}
        if chart_df is not None:
            title_suffix = chart_labels.get(chart_field, chart_field)
            st.subheader(f"å…§å»ºå½±è©•è³‡æ–™åœ– Â· {title_suffix}")
            chart_data = chart_df.rename(columns={"label": "åˆ†é¡", "count": "ç¯‡æ•¸"})
            chart = (
                alt.Chart(chart_data)
                .mark_bar(cornerRadiusTopLeft=6, cornerRadiusTopRight=6)
                .encode(
                    x=alt.X("åˆ†é¡:N", sort="-y", title="åˆ†é¡"),
                    y=alt.Y("ç¯‡æ•¸:Q", title="ç¯‡æ•¸"),
                    color=alt.Color(
                        "åˆ†é¡:N",
                        legend=None,
                        scale=alt.Scale(scheme="teals"),
                    ),
                    tooltip=["åˆ†é¡:N", "ç¯‡æ•¸:Q"],
                )
                .properties(height=320, title=f"å…§å»ºå½±è©•è³‡æ–™åœ– Â· {title_suffix}")
            )
            st.altair_chart(chart, use_container_width=True)
        if show_raw_table and chart_source:
            st.dataframe(
                pd.DataFrame(chart_source)[
                    ["id", "title", "sentiment", "topic", "rating"]
                ]
            )


if __name__ == "__main__":
    main()
